import requests
from bs4 import BeautifulSoup
from crawler.settings import HEADERS, TIMEOUT
from crawler.utils.logger import Logger
from utils.file_manager import FileManager

class ExampleSpider:
    def __init__(self):
        self.logger = Logger("example_spider")
        self.file_manager = FileManager()
        self.data = []  # í¬ë¡¤ë§ ë°ì´í„° ì €ì¥

    def fetch(self, url):
        """ì›¹ í˜ì´ì§€ ìš”ì²­ ë° HTML ê°€ì ¸ì˜¤ê¸°"""
        try:
            response = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
            response.raise_for_status()
            return response.text
        except requests.exceptions.RequestException as e:
            self.logger.error(f"Failed to fetch {url}: {e}")
            return None

    def parse(self, html):
        """HTML ë°ì´í„° íŒŒì‹±"""
        soup = BeautifulSoup(html, "html.parser")
        title = soup.title.string if soup.title else "No Title"
        return {"title": title}

    def crawl(self):
        """í¬ë¡¤ë§ ì‹¤í–‰"""
        from crawler.urls import URLS

        for url in URLS:
            self.logger.info(f"Crawling {url}...")
            html = self.fetch(url)
            if html:
                data = self.parse(html)
                self.logger.info(f"Extracted Data: {data}")
                
    def scrape(self):
        """ìƒ˜í”Œ í¬ë¡¤ë§ ë°ì´í„°"""
        self.data = [
            {"title": "ë°ì´í„° 1", "url": "https://example.com/1"},
            {"title": "ë°ì´í„° 2", "url": "https://example.com/2"}
        ]
        print("ğŸ•· í¬ë¡¤ë§ ì™„ë£Œ!")
        
    def save_data(self):
        """í¬ë¡¤ë§ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if not self.data:
            print("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        self.file_manager.save_json("crawl_results", self.data)

    def load_data(self):
        """ì €ì¥ëœ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°"""
        data = self.file_manager.load_json("crawl_results")
        if data:
            print("ğŸ“‚ ë¶ˆëŸ¬ì˜¨ ë°ì´í„°:", data)
    
    def cleanup_data(self):
        """íŒŒì¼ ì •ë¦¬ ì‘ì—…"""
        self.file_manager.cleanup_files()

if __name__ == "__main__":
    spider = ExampleSpider()
    spider.crawl()
